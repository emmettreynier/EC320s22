---
title: "**Simple Linear Regression**"
author: |
        | Giorgi Nikolaishvili
        | EC 320: Introduction to Econometrics
        | University of Oregon
date: "Spring 2022"
output: 
  html_document:
    theme: flatly  
    includes:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, cache = T)
```

## Recap 

What we learned last week:

- Grouping/slicing data using `group_by()`
- Summarizing data using `summarize()` 
- Plotting data using `ggplot2` 

## Simple Linear Regression

Suppose we have data on (observations of) random variables $Y$ and $X$.

We would like to understand the relationship between these two variables.

We may explore the relationship between $Y$ and $X$ by estimating a regression model using (running a regression on) our data.

### Conceptual Overview

The linear regression model:

$$Y_i = \beta_0 + \beta_1 \cdot X_i + u_i$$

where 

- the index $i$ denotes the observation number, $i = 1,\ldots,N$
- $Y_i$ is the $i$-th observation of the **dependent variable** (regressand)
- $X_i$ is the $i$-th observation of the **independent variable** (regressor) 
- $Y = \beta_0 + \beta_1 \cdot X$ is the **population regression line/function**
- $\beta_0$ is the **intercept** of the population regression line
- $\beta_1$ is the **slope** of the population regression line
- $u_i$ is the **error** term (variation in $Y$ unrelated to $X$)

By applying ordinary least squares (OLS) to data on variables $Y$ and $X$, we must fit the the population regression line by estimating the intercept and slope coefficient, $\beta_0$ and $\beta_1$, respectively.

### Building Intuition through Simulation

Let's load some packages that we will need in the rest of this document.

```{r}
library(pacman)
p_load(tidyverse, pracma, gapminder, modelsummary, broom)
```

Let's simulate some data on the following random variables:

- $X \sim N(5, 9)$  
- $Y = 5 + 2 X + u$, where $u \sim N(0,1)$ 

$X$ is a normally-distributed random variable with $E(X) = 5$ and $Var(X) = 9$.

It might also be useful to see that $Y$ is a normally-distributed random variable with 
$$E(Y) \\ = E(5 + 2 X + u) \\ = 5 + 2 E(X) + E(u) \\ = 5 + 2 (5) + (0) \\ = 15 \, , $$ 
and 
$$Var(Y) \\ = Var(5 + 2 X + u) \\ = Var(5) + Var(2 X) + Var(u) \\ = (0) + 2^2 Var(X) + Var(u) \\ = 4 (9) + (1) \\ = 37 \, .$$
In other words, $Y \sim N(15, 37)$.

**Important:** $Y$ is a function of $X$ and some (unobservable) noise $u$.
So, a portion of the variation in $Y$ is a reflection of the variation in $X$. 

If we have some data on (observations of) $Y$ and $X$, then we can run a regression to estimate approximately how much variation in $Y$ is caused by $X$, and what the nature of the relationship between these two variables is.

Let's simulate some data from this data-generating process, and group the observables ($Y$ and $X$) into a single dataset.

```{r}
# Number of observations 
n = 300

# `n` observations of random variable `X`
# with mean 5 and std. dev 3
X <- 5 + 3 * randn(n, 1)

# `n` observations of random variable `u`
# with mean 0 and std. dev 1
u <- randn(n, 1)

# `n` observations of random variable `Y`
# such that Y = 5 + 2 * X + u 
beta0 <- 5
beta1 <- 2
Y <- beta0 + beta1 * X + u

# Create dataset with Y, X
data_df <- tibble(Y, X)
```

Let's make histograms of the observations of $u$, $X$, and $Y$ (although, let's not forget that we're assuming $u$ is unobservable!):

```{r}
# Plot histogram of `u`
ggplot(data_df, aes(u)) +
  geom_histogram(fill = "blue", alpha = 0.6) +
  geom_vline(xintercept = 0, size = 1, linetype = "dotted") +
  theme_minimal()

# Plot histogram of `X`
ggplot(data_df, aes(X)) +
  geom_histogram(fill = "green", alpha = 0.6) +
  geom_vline(xintercept = 5, size = 1, linetype = "dotted") +
  theme_minimal()

# Plot histogram of `Y`
ggplot(data_df, aes(Y)) +
  geom_histogram(fill = "red", alpha = 0.6) +
  geom_vline(xintercept = 15, size = 1, linetype = "dotted") +
  theme_minimal()
```

The samples seem to match our specified data-generating process.

Let's explore a bit further by creating a bunch of scatter plots.

First we create a scatter plot of $u$ and $X$ (once again, recall that $u$ is assumed to be unobservable -- in reality we wouldn't be able to create this plot):

```{r}
# Scatter plot of `X` and `u`
ggplot(data_df, aes(x = u, y = X)) +
  geom_point(color = "blue", size = 2.5, alpha = 0.6) +
  theme_minimal() +
  geom_hline(yintercept = mean(X), linetype="dashed", color = "red", alpha = 0.5, size = 1) +
  geom_vline(xintercept = mean(u), linetype="dashed", color = "red", alpha = 0.5, size = 1) 
```

Next, we plot $u$ and $Y$:

```{r}
# Scatter plot of `Y` and `u`
ggplot(data_df, aes(x = u, y = Y)) +
  geom_point(color = "blue", size = 2.5, alpha = 0.6) +
  theme_minimal() +
  geom_hline(yintercept = mean(Y), linetype="dashed", color = "red", alpha = 0.5, size = 1) +
  geom_vline(xintercept = mean(u), linetype="dashed", color = "red", alpha = 0.5, size = 1) 
```

Lastly, we plot the observations on the two observable variables in our sample, $X$ and $Y$: 

```{r}
# Scatter plot of `Y` and `X`
p = ggplot(data_df, aes(x = X, y = Y)) +
      geom_point(color = "blue", size = 2.5, alpha = 0.6) +
      theme_minimal() +
      geom_hline(yintercept = mean(Y), linetype="dashed", color = "red", alpha = 0.3, size = 1) +
      geom_vline(xintercept = mean(X), linetype="dashed", color = "red", alpha = 0.3, size = 1) 

p
```

Notice that unlike the previous plots, this one shows an unmistakable positive relationship between $Y$ and $X$ -- when $X$ is high, $Y$ tends to be high, and when $X$ is low so is $Y$. 

A linear regression model essentially represents a line that captures the average relationship between $Y$ and $X$.

Recall the formula for a line representing the relationship between some variables $y$ and $x$: $y = f(x) = a + b x$. 

Here $a$ is the intercept coefficient, and $b$ is the slope coefficient. 

Let's try to eyeball a best-fit line to the given data. 

We start by guessing $a = 0$ and $b = 1$:

```{r}
# Scatter plot of `Y` and `X`
p +
  geom_abline(intercept = 0, slope = 1, color = "black", size = 1.5, alpha = 0.6) # Plot line w/ a = 0, b = 1
```

Clearly the above line is not the line of best-fit.

What if we shift it up a little bit by setting $a = 5$ intead of $a = 0$:

```{r}
# Scatter plot of `Y` and `X`
p +
  geom_abline(intercept = 5, slope = 1, color = "black", size = 1.5, alpha = 0.6)
```

The above is better, but far from perfect.

Now why don't we keep $a = 5$ and change $b$ from $1$ to $3$: 

```{r}
# Scatter plot of `Y` and `X`
p +
  geom_abline(intercept = 5, slope = 3, color = "black", size = 1.5, alpha = 0.6)
```

Clearly, the slope of the best-fit line seems to be between $1$ and $3$, so let's try $2$:

```{r}
# Scatter plot of `Y` and `X`
p +
  geom_abline(intercept = 5, slope = 2, color = "black", size = 1.5, alpha = 0.6)
```

Great -- the line of best-fit *must* look something similar to the above. 

Now let's plot the above line along with a line with $a = \beta_0$ and $b = \beta_1$ -- in other words, we set the line coefficients equal to the population regression coefficients $\beta_0$ and $\beta_1$ from the actual data-generating process.

```{r}
# Scatter plot of `Y` and `X`
p +
  geom_abline(intercept = 5, slope = 2, color = "green", size = 5, alpha = 0.3) +
  geom_abline(intercept = beta0, slope = beta1, color = "red", size = 2, alpha = 0.8)
```

These two lines overlap perfectly.

Why? Because we've set $a = \beta_0 = 5$ and $b = \beta_1 = 2$. 

In other words, we've successfully recovered the population coefficients parameters by eyeballing the line of best fit to the data.

Of course, it isn't always so easy, so we want to have a statistically sound method of carrying out the above process without ever eyeballing any of the true model parameters. 

## Estimating Coefficients

The ordinary least squares (OLS) estimator finds regression coefficients that minimize the sum of squared errors in predicting $Y$ given $X$.

The true data-generating process:
$$ Y = \beta_0 + \beta_1 \cdot X + \varepsilon \, .$$

The estimated data-generating process conditional on observations of $X$:
$$ Y = b_0 + b_1 \cdot X + e \, .$$

$b_0$ is an estimate of $\beta_0$, and $b_1$ is an estimate of $\beta_1$. 

OLS computes $b_0$ and $b_1$ such that $\sum_{i=1}^N e_i^2 = \sum_{i=1}^N (y_i - b_0 - b_1 \cdot x_i)^2$ is minimized.

You can think of $\sum_{i=1}^N e_i^2$ as a measure of closeness between the fitted line (implicitly generated using OLS through the estimation of regression coefficients) and the data.

The red line in the following graph is a regression line:

```{r}
p +
  geom_smooth(method = "lm", se = FALSE, color = "red", size = 1.5, alpha = 0.6)
```

The OLS estimators of $\beta_0$ and $\beta_1$ are the following:

$$ b_0 = \overline{Y} - b_1 \overline{X} $$

$$ b_1 = \frac{\sum_{i=1}^N (X_i - \overline{X}) (Y_i - \overline{Y}) }{\sum_{i=1}^N (X_i - \overline{X})^2} $$
Let's compute these manually:

```{r}
b1 <- sum((X - mean(X)) * (Y - mean(Y))) / sum((X - mean(X))^2)
b0 <- mean(Y) - b1 * mean(X) 
print(b0)
print(b1) 
```

Luckily, the `lm()` function allows us to run OLS and estimate linear regression coefficients, as well as generate standard errors for each of the coefficient estimates:

```{r}
mod <- lm(formula = Y ~ X, data = data_df)
mod
```

The `lm()` function has two necessary inputs: `data` and `formula`. 

- `data` refers to the dataset containing our variables of interest;
- `formula` specifies the actual regression model. `Y ~ X` is equivalent to specifying $Y = \beta_0 + \beta_1 X + \varepsilon$. 

Notice that we've saved our model estimates as an object called `mod`.

When we print `mod`, we see the regression coefficient estimates, but not much else. 

A great way of printing the full relevant set of information associated with estimating a regression model is the `modelsummary()` function.

We simply give the `mod` object as in input to `modelsummary()`, and it spits out a nice-looking table.

```{r}
modelsummary(mod)
```

You should use `modelsummary()` to present your model estimates to a reader -- all other formats are too messy.

`modelsummary()` is what academics use to present their results in peer-reviewed papers! You should follow their lead.

## Hypothesis Testing 

Within the context of regression modeling, a hypothesis test is a rule that specifies whether to accept or reject a claim about a population parameter ($\beta_0$, $\beta_1$). 

There must be two opposing hypotheses:

- $H_0$: null hypothesis ($\beta_i = \beta_{i,0}$)
- $H_1$: alternative hypothesis ($\beta_i \neq \beta_{i,0}$)

The coefficient estimates and standard errors generated using OLS provide us with sufficient evidence to perform hypothesis tests on the true values of $\beta_0$ and $\beta_1$. 

Using the fact that the estimates $b_0$ and $b_1$ are approximately normally distributed in large samples, testing hypotheses about the true values of $\beta_0$ and $\beta_1$ can be performed using $t$-statistics:

$$ t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}} = \frac{b_i - \beta_{i,0}}{SE(b_i)} \, .$$

Given a two-sided alternative hypothesis, we reject at the 5$\%$ level if $t > 1.96$ ($p$-value is less than 0.05). 

We may retreive all information relevant for hypothesis tests from a regression output using the `summary()` function.

In the following code chunk, we give the `mod` object (containing the regression output from regressing $Y$ on $X$) to the `summary()` function, and save the output as `mod_summ`. 

```{r}
mod_summ <- summary(mod)
mod_summ 
```

Notice that the above object has the following "fields": 

- Call 
- Residuals
- Coefficients
- Signif. codes
- Residual standard error
- Multiple R-squared
- F-statistics

The one we care about is "Coefficients" (retrievable as `coefficients`): 

```{r}
coefs <- mod_summ$coefficients
coefs 
```

Using this information, let's test the following hypothesis:

- $H_0$: $\beta_1 = 3$ 
- $H_1$: $\beta_1 \neq 3$ 

```{r}
# Specify hypothesized value of beta1
h0_beta1      = 3

# Retrieve estimated value of beta1
est_beta1     = coefs[2,1] # 2nd row (X), 1st column (Estimate)

# Retrieve standard error of beta1 estimate
std_err_beta1 = coefs[2,2] # 2nd row (X), 2nd column (Std. Error) 

# Compute t-statistic of the test
t_stat        = (h0_beta1 - est_beta1) / std_err_beta1

# Print the computed t-statistic 
t_stat 
```

This is a very high t-statistic! 

Recall that given a two-sided alternative hypothesis, we reject at the 5$\%$ level if $t > 1.96$ ($p$-value is less than 0.05). 

Therefore, we may reject the null hypothesis at (at least) a 5$\%$ level of confidence. 
