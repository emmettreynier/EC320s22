---
title: "Midterm Review"
subtitle: "EC 320: Introduction to Econometrics"
author: "Emmett Saulnier"
date: "Spring 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(ggthemes, viridis, knitr, extrafont, tidyverse, magrittr, wooldridge, stargazer, latex2exp, parallel, broom, ggforce)
# Define colors
red_pink <- "#EC7662"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
met_slate <- "#23373b" # metropolis font color
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  #dpi = 300,
  #cache = T,
  warning = F,
  message = F
)  
theme_simple <- theme_bw() + theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  text = element_text(color = met_slate, size = 14),
  axis.text.x = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  legend.position = "none"
)
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
# Neumark data
data <- get(data(wage2))
lm_e <- lm(lwage ~ educ, data = data)
b0 <- lm_e$coefficients[1]
b1 <- lm_e$coefficients[2]
r_2 <- summary(lm(lwage ~ educ, data = data))$r.squared
```

# Prologue

---

# Housekeeping

### Midterm Thursday (4/28)     
  - 5 multiple choice, 5 true/false, and 5 short answer questions  
    - 5 points each for multiple choice and true/false   
    - 10 points each for short answer questions (each one has multiple parts)   
  - Study materials...
    1. The lecture slides and your notes  
    2. Homework problems
    3. Textbook reading:  
  .hi-pink[ITE] Chapters Review, 1, and 2.1-2  
  .hi-green[MM] Chapters 1 and 2  
  - Bring a calculator if you have one (I will have extras, but not 60)  
  - Review session in Lab tomorrow, come with questions!  

---
class: inverse, middle  

# Statistical Review

---
# Summation and Expectation Rules  

If $\{(x_i, y_i): 1, \dots, n \}$ is a set of $n$ pairs, and $a$ and $b$ are constants, then $$\sum_{i=1}^{n} (ax_i + by_i) = a \sum_{i=1}^{n} x_i + b \sum_{i=1}^{n} y_i.$$

If $\{a_1, a_2, \dots , a_n\}$ are constants and $\{X_1, X_2, \dots , X_n\}$ are random variables, then

$$
\color{#FD5F00}{\mathop{\mathbb{E}}(a_1 X_1 + a_2 X_2 + \dots + a_n X_n)} = \color{#007935}{a_1 \mathop{\mathbb{E}}(X_1) + a_2 \mathop{\mathbb{E}}(X_2) + \dots + a_n \mathop{\mathbb{E}}(X_n)}.
$$

---
# Variance and Covariance Rules  

For constants $a$, $b$, $c$, and $d$

$$
\mathop{\text{Var}} (aX + bY + d) = a^2 \mathop{\text{Var}}(X) + b^2 \mathop{\text{Var}}(Y) + 2ab\mathop{\text{Cov}}(X, Y).
$$
Similarly, we have the following covariance rule

$$
\mathop{\text{Cov}}(aX + b, cY + d) = ac\mathop{\text{Cov}}(X, Y)
$$

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

```{R, gen dataset, include = F, cache = T}
# Set population and sample sizes
n_p <- 100
n_s <- 10
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 2, sd = 20),
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Means
m0 <- mean(pop_df$x)
m1 <- mean(subset(pop_df$x, pop_df$s1 == T))
m2 <- mean(subset(pop_df$x, pop_df$s2 == T))
m3 <- mean(subset(pop_df$x, pop_df$s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 1, X = 1:1e4, FUN = function(x, size = n_s) {
  pop_df %>% 
    sample_n(size = size) %>% 
    summarize(mu_hat = mean(x))
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "#195c23", size = 10) +
theme_empty
```

.center[**Population**]

]

--

.pull-right[

```{R, mean1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "#195c23", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  theme_empty
```

.center[**Population relationship**]
<br>
$\mu = `r round(m0, 2)`$

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "#ffa600", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 1:** 10 random individuals]

]

--

.pull-right[

```{R, sample1 mean, echo = F, fig.fullwidth = T, dev = "svg", message=FALSE}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "#195c23", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  geom_histogram(data = subset(pop_df, s1 == T), aes(x), fill = "#ffa600", alpha = 0.40) +
  geom_vline(xintercept = m1, size = 2, color = "#ffa600") +
  theme_empty
```

.center[

**Population relationship**
<br>
$\mu = `r round(m0, 2)`$

**Sample relationship**
<br>
$\hat{\mu} = `r round(m1, 2)`$

]

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "#ffa600", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 2:** 10 random individuals]

]

--

.pull-right[

```{R, sample2 mean, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "#195c23", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "#195c23") +
  geom_histogram(data = subset(pop_df, s2 == T), aes(x), fill = "#ffa600", alpha = 0.50) +
  geom_vline(xintercept = m2, size = 2, color = "#ffa600") +
  theme_empty
```

.center[

**Population relationship**
<br>
$\mu = `r round(m0, 2)`$

**Sample relationship**
<br>
$\hat{\mu} = `r round(m2, 2)`$

]

]


---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 1: Unbiasedness.**

.pull-left[

**Unbiased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\mu} \right] = \mu$

```{R, unbiased pdf, echo = F, dev = "svg"}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = red_pink, alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX(r"($\mu$)")) +
scale_y_continuous(expand = c(0, 0), limits = c(0, NA))+
theme_bw()+ 
theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  axis.text.x = element_text(size = 40),
  axis.text.y = element_blank(),
  axis.title = element_blank(),
  line = element_blank()
)
```

]

--

.pull-right[

**Biased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\mu} \right] \neq \mu$

```{R, biased pdf, echo = F, dev = "svg"}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX(r"($\mu$)")) +
scale_y_continuous(expand = c(0, 0), limits = c(0, NA))+
theme_bw()+ 
theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  axis.text.x = element_text(size = 40),
  axis.text.y = element_blank(),
  axis.title = element_blank(),
  line = element_blank()
)
```

]

---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 2: Low Variance (a.k.a. Efficiency).**

```{R, variance pdf, echo = F, dev = "svg", fig.height = 5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX(r"($\mu$)")) +
scale_y_continuous(expand = c(0, 0), limits = c(0, NA))+
theme_bw()+ 
theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  axis.text.x = element_text(size = 20),
  axis.text.y = element_blank(),
  axis.title = element_blank(),
  line = element_blank()
)
```


---
# Hypothesis Testing

Reject H.sub[0] if $\left| z \right| =\left| \dfrac{\hat{\mu} - \mu_0}{\mathop{\text{sd}}(\hat{\mu})} \right| > z_{crit}$. Where $z_{crit}$ comes from the normal distribution and the significance of the test you would like to run. 

```{r, echo = F, dev = "svg", fig.height = 3.75}
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dnorm(seq(-4,4, by = 0.01))
)
crit <- qnorm(c(.025,.975))
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0), breaks = c(-1.96, 0, 1.96), labels = c(TeX(r"($\mu_0 - 1.96 \, s.d.$)"), TeX(r"($\mu_0$)"), TeX(r"($\mu_0 + 1.96 \, sd$)"))) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = red_pink) +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = red_pink) +
  geom_polygon(data = df %>% filter(x <= qnorm(1 - 0.975) & x >= qnorm(0.975)), aes(x, y), fill = red_pink) +
  geom_vline(xintercept = qnorm(0.975), size = 0.35, linetype = "dashed", color = met_slate) +
  geom_vline(xintercept = qnorm(1 - 0.975), size = 0.35, linetype = "dashed", color = met_slate) +
  theme_bw()+ 
theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  axis.text.x = element_text(),
  axis.text.y = element_blank(),
) + 
  xlab("") + 
  ylab("") 
```

If you observe an extreme value in the data relative to the **assumed** null distribution, then the null hypothesis is probably not correct! 


---
class: inverse, middle  

# Fundamental Problem of Econometrics 


---
# Fundamental Problem of Econometrics

## Ideal comparison
$$
\begin{align}
  \tau_i = \color{#EC7662}{y_{1,i}} &- \color{#9370DB}{y_{0,i}}
\end{align}
$$

Highlights the fundamental problem of econometrics, much like when a traveller assesses options down two separate roads.

--

## The problem

- If we observe $\color{#EC7662}{y_{1,i}}$, then we cannot observe $\color{#9370DB}{y_{0,i}}$.

- If we observe $\color{#9370DB}{y_{0,i}}$, then we cannot observe $\color{#EC7662}{y_{1,i}}$.

- Can only observe what actually happened; cannot observe the **counterfactual**.

---
# Solutions to the fundamental problem  

A dataset that we can observe for 10 people looks something like
.pull-left[
```{R, ideal_data_obs, echo = F}
set.seed(3)
ideal_df <- data.frame(
  i = 1:10,
  trt = rep(c(1, 0), each = 5),
  y1i = c(runif(10, 4, 10) %>% round(2)),
  y0i = c(runif(10, 0, 5) %>% round(2))
)

obs_df <- ideal_df
obs_df$y0i[1:5] <- NA
obs_df$y1i[6:10] <- NA
obs_df
```
]

--

.pull-right[
We can't observe $\color{#EC7662}{y_{1,i}}$ and $\color{#9370DB}{y_{0,i}}$.

But, we do observe
- $\color{#EC7662}{y_{1,i}}$ for $i$ in 1, 2, 3, 4, 5
- $\color{#9370DB}{y_{0,i}}$ for $i$ in 6, 7, 8, 9, 10

]

--

We "fill in" the `NA`s and estimate $\overline{\tau}$ using **RCT**'s, natural experiments, or controlling for selection bias.  

---
class: inverse, middle  

# Regression

---
# The Regression Model

We can estimate the effect of $X$ on $Y$ by estimating a .hi[regression model]:

$$Y_i = \beta_0 + \beta_1 X_i + u_i$$

- $Y_i$ is the outcome variable.

--

- $X_i$ is the treatment variable (continuous).

--

- $\beta_0$ is the **intercept** parameter. $\mathop{\mathbb{E}}\left[ {Y_i | X_i=0} \right] = \beta_0$

--

- $\beta_1$ is the **slope** parameter, which under the correct causal setting represents marginal change in $X_i$'s effect on $Y_i$. $\frac{\partial Y_i}{\partial X_i} = \beta_1$


--

- $u_i$ is an error (disturbance) term that includes all other (omitted) factors affecting $Y_i$.

---
# OLS

<br>

The __OLS estimator__ chooses the parameters $\hat{\beta_1}$ and $\hat{\beta_2}$ that minimize the .hi[residual sum of squares (RSS)]:

$$\min_{\hat{\beta}_1,\, \hat{\beta}_2} \quad \color{#EC7662}{\sum_{i=1}^n \hat{u}_i^2}$$

Where $\hat{u}_i = y_i - \hat{y}_i$ for fitted values $\hat{y}_i = \hat{\beta}_1 + \hat{\beta}_2x_i$. Thus, the problem becomes

$$\min_{\hat{\beta}_1,\, \hat{\beta}_2} \quad \color{#EC7662}{\sum_{i=1}^n (y_i - \hat{\beta}_1 - \hat{\beta}_2x_i)^2}$$

---
# OLS Formulas

<br>

For details, see the [handout](https://github.com/emmettsaulnier/EC320s22/blob/main/lectures/06-simple-reg-1/Handout-01.pdf) posted on Canvas.

__Slope coefficient__

$$\hat{\beta}_2 = \dfrac{\sum_{i=1}^n (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^n  (X_i - \bar{X})^2}$$

__Intercept__

$$ \hat{\beta}_1 = \bar{Y} - \hat{\beta}_2 \bar{X} $$

---
# OLS Properties

<br>

The way we selected OLS estimates $\hat{\beta}_1$ and $\hat{\beta}_2$ gives us three important properties:

1. Residuals sum to zero: $\sum_{i=1}^n \hat{u}_i = 0$.

2. The sample covariance between the independent variable and the residuals is zero: $\sum_{i=1}^n X_i \hat{u}_i = 0$.

3. The point $(\bar{X}, \bar{Y})$ is always on the regression line.

---
# Goodness of Fit

What percentage of the variation in our $Y_i$ is *apparently* explained by our model? The $R^2$ term represents this percentage.

Total variation is represented by .hi-blue[TSS] and our model is capturing the 'explained' sum of squares, .hi-blue[ESS].

Taking a simple ratio reveals how much variation our model explains. 

- $R^2 = \frac{\text{ESS}}{\text{TSS}}$ varies between 0 and 1

- $R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$, 100% less the unexplained variation 



---
class: inverse, middle  

# Classical Assumptions  

---
# Classical Assumptions of OLS

1. **Linearity:** The population relationship is .hi[linear in parameters] with an additive error term.

--

2. **Sample Variation:** There is variation in $X$.

--

3. **Exogeneity:** The $X$ variable is .hi[exogenous] (*i.e.,* $\mathop{\mathbb{E}}\left( u|X \right) = 0$).<sup>.pink[†]</sup>

--

4. **Homoskedasticity:** The error term has the same variance for each value of the independent variable (*i.e.,* $\mathop{\text{Var}}(u|X) = \sigma^2$).

--

5. **Non-autocorrelation:** The values of error terms have independent distributions (*i.e.,* $E[u_i u_j]=0, \forall i \text{ s.t. } i \neq j$)

--

6. **Normality:** The population error term is normally distributed with mean zero and variance $\sigma^2$ (*i.e.,* $u \sim N(0,\sigma^2)$)

.footnote[
.pink[†] Implies assumption of **Random Sampling:** We have a random sample from the population of interest.
]

---
# When is OLS Unbiased?

## Required Assumptions

1. **Linearity:** The population relationship is .hi[linear in parameters] with an additive error term.

2. **Sample Variation:** There is variation in $X$.

3. **Exogeneity:** The $X$ variable is .hi[exogenous] (*i.e.,* $\mathop{\mathbb{E}}\left( u|X \right) = 0$).

--

&#9755; (3) implies **Random Sampling**. Without, the internal validity of OLS uncompromised, but our external validity becomes uncertain.<sup>.pink[†]</sup>

.footnote[
.pink[†] **Internal Validity:** relates to how well a study is conducted (does it satisfy OLS assumptions?).<br> **External Validity:** relates to how applicable the findings are to the real world.
]

---
# Exogeneity (A3.)

## Assumption

The $X$ variable is __exogenous:__ $\mathop{\mathbb{E}}\left( u|X \right) = 0$.

- For _any_ value of $X$, the mean of the error term is zero.

.hi[The most important assumption!]

--

Really two assumptions bundled into one:

1. On average, the error term is zero: $\mathop{\mathbb{E}}\left(u\right) = 0$.

2. The mean of the error term is the same for each value of $X$: $\mathop{\mathbb{E}}\left( u|X \right) = \mathop{\mathbb{E}}\left(u\right)$.

---
# Exogeneity (A3.)

## Assumption

The $X$ variable is __exogenous:__ $\mathop{\mathbb{E}}\left( u|X \right) = 0$.

- The assignment of $X$ is effectively random.
- **Implication:** .hi-purple[no selection bias] and .hi-green[no omitted-variable bias].


---
exclude: true

```{R generate pdfs, include = F, eval = F}
#remotes::install_github('rstudio/pagedown')
library(pagedown)
pagedown::chrome_print("midterm-review.html", output = "midterm-review.pdf")
```